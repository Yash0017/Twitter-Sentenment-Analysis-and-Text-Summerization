{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import language_tool_python\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "\n",
    "tweets_df = pd.read_csv('tweets.csv', header = None)\n",
    "headers = ['All_tweets']\n",
    "tweets_df.columns = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = tweets_df['All_tweets']\n",
    "article_text = \"\"\n",
    "for p in tweets_df['All_tweets']:\n",
    "    article_text += p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "#formatted_article_text = re.sub(r'\\s+', ' ', re.sub('[^a-zA-Z]', ' ', re.sub(r'\\s+', ' ', re.sub(r'\\[[0-9]*\\]', ' ', article_text)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') only once\n",
    "sentence_list = nltk.sent_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') only once\n",
    "stopwords = stopwords.words('english')\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequncy = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPPU Nothing is important than students livesManWoman student so I request shri narendramodi ji to cancel all final year exams SPPU demands late fees from students So please Cancel Exam By charging late fees from students SPPU is Pressurising students Sukhadak Mirror reports from Pune SPPU s order to vacate hostels comes as a shock for students who are no longer in Pune and preparing for their final exams If decision regarding exams is not taken in due time students will start loosing placement offers they have There is a widespread uncertainty among the final year students which should be addressed immediately Students from out of town arrive in Pune seeking sable internet connections reference books to take online exam \n"
     ]
    }
   ],
   "source": [
    "#Getting the Summary\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', summary )\n",
    "text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet):     \n",
    "    analysis = TextBlob(tweet) \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 'neutral'\n",
    "    else: \n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(): \n",
    "    tweets = [] \n",
    "    for tweet in sentence_list: \n",
    "        parsed_tweet = {} \n",
    "        parsed_tweet['text'] = tweet \n",
    "        parsed_tweet['sentiment'] = get_tweet_sentiment(tweet) \n",
    "        if parsed_tweet not in tweets: \n",
    "            tweets.append(parsed_tweet)  \n",
    "  \n",
    "           \n",
    "    return tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.092436974789916\n",
      "23.529411764705884\n",
      "45.378151260504204\n"
     ]
    }
   ],
   "source": [
    "tweets = get_tweets() \n",
    "ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']\n",
    "positive = 100*len(ptweets)/len(tweets)\n",
    "print(positive) \n",
    "ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "negative = 100*len(ntweets)/len(tweets)\n",
    "neutral = 100*(len(tweets) -(len( ntweets )+len( ptweets)))/len(tweets)\n",
    "print(negative) \n",
    "print(neutral) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"text\": str(text),\n",
    "    \"positive\": str(positive),\n",
    "    \"negative\": str(negative),\n",
    "    \"neutral\": str(neutral),\n",
    "}\n",
    "\n",
    "outputfile = open(\"project_output.json\", \"w\")\n",
    "json.dump(result, outputfile)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
